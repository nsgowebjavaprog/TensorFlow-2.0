{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ae2bdb",
   "metadata": {},
   "source": [
    "Hereâ€™s a comprehensive and detailed explanation of âœ… 5. Distributed Training & Performance, which is critical for building scalable and production-grade machine learning models using TensorFlow. Understanding distributed training, GPU/TPU optimization, and performance profiling is essential for real-world machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcc83f",
   "metadata": {},
   "source": [
    "âœ… 5. Distributed Training & Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f9bb2",
   "metadata": {},
   "source": [
    "ðŸ”¹ 1. tf.distribute Strategies\n",
    "\n",
    "\n",
    "TensorFlow offers several distribution strategies to scale training across multiple devices (e.g., GPUs, TPUs, multiple nodes). These strategies ensure that large models are trained efficiently in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3a497",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "MirroredStrategy enables synchronous training where each device computes the gradient on its own data, but the gradients are averaged across all devices to update the model parameters. It's best used when your model is too large to fit into the memory of a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36045e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Instantiate MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085afb80",
   "metadata": {},
   "source": [
    "ðŸ“˜ 1.2 MultiWorkerStrategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e0a9f",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "MultiWorkerStrategy enables distributed training across multiple machines. It is essential for large-scale training where a single machine is not sufficient to process the data or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the MultiWorkerStrategy\n",
    "strategy = tf.distribute.MultiWorkerStrategy()\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Build the model as usual\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13772597",
   "metadata": {},
   "source": [
    "ðŸ”¹ 2. Mixed Precision Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583c8fc",
   "metadata": {},
   "source": [
    "ðŸ“˜ 2.1 Mixed Precision Training\n",
    "\n",
    "\n",
    "Mixed precision training uses both 16-bit (half precision) and 32-bit (single precision) floating point numbers to speed up training and reduce memory usage, without losing model accuracy.\n",
    "\n",
    "Benefits: Faster training, reduced memory usage, and less computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db3a09",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "Mixed precision allows TensorFlow to use 16-bit precision for most computations while using 32-bit precision only when necessary, for example in the output layer or loss calculations. This reduces memory footprint and increases throughput, making the model training faster, especially on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Set mixed precision policy\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Build and compile the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', dtype='float32')  # Last layer in float32\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ecb55",
   "metadata": {},
   "source": [
    "ðŸ”¹ 3. TPU/GPU Optimization\n",
    "\n",
    "ðŸ“˜ 3.1 TPU Optimization\n",
    "\n",
    "TPUs (Tensor Processing Units) are specialized hardware accelerators developed by Google for high throughput training of machine learning models. TensorFlow provides integration with Google Cloud TPUs, allowing you to speed up training.\n",
    "\n",
    "How to use TPUs:\n",
    "\n",
    "Set up a TPU-enabled environment (e.g., Google Colab or Google Cloud AI Platform).\n",
    "\n",
    "Use the tf.distribute.Strategy APIs to leverage TPUs.\n",
    "\n",
    "Ensure that you use mixed precision for TPUs to take advantage of faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a61684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the strategy for TPU\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://<TPU_ADDRESS>')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.config.experimental_connect_to_host('grpc://<TPU_ADDRESS>')\n",
    "\n",
    "# Use TPU strategy\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "# Build and compile the model inside strategy scope\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on TPU\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606233d",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "TPUs provide an order of magnitude faster training compared to GPUs for large-scale models. TensorFlowâ€™s integration with TPUs simplifies the process of scaling training for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec363225",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77f8460",
   "metadata": {},
   "source": [
    "ðŸ“˜ 3.2 GPU Optimization\n",
    "To optimize performance on GPUs, ensure you use:\n",
    "\n",
    "CUDA and cuDNN for hardware acceleration.\n",
    "\n",
    "TensorFlow GPU version to enable high-performance computing.\n",
    "\n",
    "ðŸ”¸ Code Example (GPU Usage):\n",
    "TensorFlow automatically detects and uses GPUs, but you can manually set the GPU for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "# Set GPU memory growth to avoid memory allocation errors\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e945f7",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "GPU Optimization ensures that TensorFlow utilizes GPU memory efficiently, improving training times. Memory growth ensures that TensorFlow allocates memory only as needed, preventing memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ 4. Performance Profiling using tf.profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f391d",
   "metadata": {},
   "source": [
    "ðŸ“˜ 4.1 tf.profiler\n",
    "\n",
    "\n",
    "TensorFlow Profiler helps you identify performance bottlenecks in the model, such as slow operations or poor memory usage. It provides insights into model performance for both training and inference phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e92bf",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "tf.profiler provides valuable information about where time and memory are being spent during training. This helps you optimize performance by identifying slow operations or resource bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create the profiler callback\n",
    "profiler_callback = tf.keras.callbacks.Profiling(\n",
    "    log_dir='./logs', profile_batch=2, show_memory=True)\n",
    "\n",
    "# Train the model with profiler\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64, callbacks=[profiler_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e174fc",
   "metadata": {},
   "source": [
    "| Concept                          | Why It's Important |\n",
    "|----------------------------------|--------------------|\n",
    "| **MirroredStrategy**             | Scales model training across multiple GPUs, synchronizing gradients. |\n",
    "| **MultiWorkerStrategy**          | Scales training across multiple machines (nodes), ideal for large models. |\n",
    "| **Mixed Precision Training**     | Speeds up training by using **16-bit precision** for most operations. |\n",
    "| **TPU Optimization**             | Leverages **TPUs** for faster training on large models, especially in cloud environments. |\n",
    "| **GPU Optimization**             | Optimizes performance for training on **GPUs**, reducing memory overhead and speeding up execution. |\n",
    "| **Performance Profiling (`tf.profiler`)** | Helps identify **training bottlenecks** and optimize resource usage. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
