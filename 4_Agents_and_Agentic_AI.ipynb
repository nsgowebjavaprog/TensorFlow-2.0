{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c6dd9b",
   "metadata": {},
   "source": [
    "âœ… 6. TensorFlow Agents & Agentic AI (RL + Tool Use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f54a6",
   "metadata": {},
   "source": [
    "Agentic AI roles focus on autonomous, decision-making agents that interact with environments, learn policies, and sometimes use external tools (e.g., search, calculators, databases) to enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a8e01",
   "metadata": {},
   "source": [
    "ðŸ”¹ 1. TensorFlow Agents (TF-Agents Library)\n",
    "\n",
    "TF-Agents is TensorFlow's official reinforcement learning library. It provides modular and extensible components for building complex RL pipelines.\n",
    "\n",
    "âœ… Core Concepts in TF-Agents:\n",
    "TFEnvironment: A wrapper around a standard environment (e.g., Gym).\n",
    "\n",
    "Policy: A strategy the agent uses to choose actions.\n",
    "\n",
    "ReplayBuffer: Stores past experiences for off-policy learning.\n",
    "\n",
    "Algorithms: Includes DQN, PPO, DDPG, SAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae59df7",
   "metadata": {},
   "source": [
    "ðŸ“Œ Example: DQN on CartPole using TF-Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32313b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# ---------------------- 1. Load the Gym Environment ----------------------\n",
    "# Load the CartPole environment from OpenAI Gym\n",
    "# Wrap it in a TF-compatible environment for training and evaluation\n",
    "env_name = 'CartPole-v0'\n",
    "train_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "\n",
    "# ---------------------- 2. Define the Q-Network ----------------------\n",
    "# The Q-Network is a simple feedforward neural network that predicts Q-values\n",
    "# fc_layer_params = tuple indicating the number of units in each hidden layer\n",
    "fc_layer_params = (100,)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),  # Observation space specification\n",
    "    train_env.action_spec(),       # Action space specification\n",
    "    fc_layer_params=fc_layer_params  # Hidden layer sizes\n",
    ")\n",
    "\n",
    "# ---------------------- 3. Set up the DQN Agent ----------------------\n",
    "# Define the optimizer and global training step\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# Create the DQN agent using the environment specs and Q-network\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),   # Time step specification\n",
    "    train_env.action_spec(),      # Action specification\n",
    "    q_network=q_net,              # Q-network to predict Q-values\n",
    "    optimizer=optimizer,          # Optimizer for training\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,  # Loss function\n",
    "    train_step_counter=global_step  # Counter for training steps\n",
    ")\n",
    "\n",
    "# Initialize agent variables\n",
    "agent.initialize()\n",
    "\n",
    "# ---------------------- 4. Set up the Replay Buffer ----------------------\n",
    "# Replay buffer stores experience tuples for training the agent\n",
    "# data_spec defines the structure of experience data\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,  # Data specification for collection\n",
    "    batch_size=train_env.batch_size,    # Batch size (should match env batch size)\n",
    "    max_length=10000                    # Max size of the buffer\n",
    ")\n",
    "\n",
    "# ---------------------- 5. Data Collection Function ----------------------\n",
    "# Function to collect a single experience step and store it in the buffer\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()          # Get current state\n",
    "    action_step = policy.action(time_step)               # Choose action using policy\n",
    "    next_time_step = environment.step(action_step.action)  # Take action in env\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)  # Create transition\n",
    "    buffer.add_batch(traj)                               # Store transition in buffer\n",
    "\n",
    "# ---------------------- 6. Initial Data Collection ----------------------\n",
    "# Collect some initial experiences to populate the replay buffer\n",
    "for _ in range(100):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "# ---------------------- 7. Sample Experience and Train the Agent ----------------------\n",
    "# Sample all experiences from the buffer and use them to train the agent\n",
    "experience = replay_buffer.gather_all()  # Retrieve all stored experiences\n",
    "agent.train(experience)                  # Train the agent using the sampled data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43901ae",
   "metadata": {},
   "source": [
    "âœ… Explanation:\n",
    "\n",
    "We define an environment (CartPole), build a Q-network, and train a DQN agent.\n",
    "\n",
    "Replay buffer helps with stability by reusing past experiences.\n",
    "\n",
    "This structure supports extending to PPO, SAC, DDPG, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eeb489",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157f867",
   "metadata": {},
   "source": [
    "ðŸ”¹ 2. Multi-Agent Systems (MAS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f27f83",
   "metadata": {},
   "source": [
    "Multi-agent systems involve multiple autonomous agents learning and interacting. TF-Agents supports custom environments with multi-agent dynamics (e.g., cooperative/competitive games like Pong, StarCraft)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key components:\n",
    "\n",
    "Multiple agents sharing or competing over rewards.\n",
    "\n",
    "Shared or independent policies.\n",
    "\n",
    "Communication protocols (optional).\n",
    "\n",
    "\n",
    "You can simulate multi-agent RL by:\n",
    "\n",
    "\n",
    "Creating multi-agent environments.\n",
    "\n",
    "Using multiple agents with their own policies and buffers.\n",
    "\n",
    "Training them using centralized or decentralized learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbef964",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6302c53",
   "metadata": {},
   "source": [
    "ðŸ”¹ 3. Toolformer-Style Agentic Frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Toolformer-style agent combines:\n",
    "\n",
    "\n",
    "LLMs (e.g., GPT or BERT).\n",
    "\n",
    "Tool usage (e.g., calculators, code execution, API calls).\n",
    "\n",
    "Reinforcement Learning for feedback-based learning.\n",
    "\n",
    "\n",
    "In TensorFlow, you can build this by:\n",
    "\n",
    "\n",
    "Using an LLM (via HuggingFace + TensorFlow).\n",
    "\n",
    "Creating tools (as environments).\n",
    "\n",
    "Using Reinforcement Learning to train the agent to choose tools and use them effectively.\n",
    "\n",
    "\n",
    "ðŸ“Œ Example Use Case:\n",
    "\n",
    "The agent has access to:\n",
    "\n",
    "A calculator tool\n",
    "\n",
    "An external API (e.g., weather, currency)\n",
    "\n",
    "The RL policy learns when and how to invoke the right tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b864b73",
   "metadata": {},
   "source": [
    "ðŸ”¹ 4. Integration with LangChain / HuggingFace Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can integrate TensorFlow with LangChain and HuggingFace Transformers to create LLM-based agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”§ Example Setup:\n",
    "Use HuggingFace Transformers with TensorFlow (e.g., BERT-TF or TFGPT2).\n",
    "\n",
    "Use LangChain for building agent toolchains:\n",
    "\n",
    "Prompt templates\n",
    "\n",
    "Tool selection\n",
    "\n",
    "Memory (history, embedding)\n",
    "\n",
    "Reinforce agent decisions using RL + TF-Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2bd57",
   "metadata": {},
   "source": [
    "Example: LLM with tool-invoking policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55414b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Invoke calculator: 3 + 5\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a431ad4",
   "metadata": {},
   "source": [
    "| Concept | Details |\n",
    "|--------|---------|\n",
    "| **TF-Agents** | Core RL library in TensorFlow for DQN, PPO, SAC, etc. |\n",
    "| **Environment** | Use Gym or custom environments via `TFEnvironment` |\n",
    "| **Policy & Buffer** | Central to training agents (on/off-policy) |\n",
    "| **Multi-Agent Systems** | Simulate agent interactions; useful for real-world environments |\n",
    "| **Toolformer** | Combine LLM + Tools + RL; mimic real-world agentic behavior |\n",
    "| **LangChain + HuggingFace** | Combine LLMs with environment/tool control via RL agent in TensorFlow |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
