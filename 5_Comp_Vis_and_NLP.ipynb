{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af14011",
   "metadata": {},
   "source": [
    "‚úÖ 7. Computer Vision & NLP in TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17212f24",
   "metadata": {},
   "source": [
    "üîπ A. Computer Vision (CV)\n",
    "\n",
    "1. Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70d049",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# ---------------------- 1. Load and Preprocess Data ----------------------\n",
    "# Load CIFAR-10 dataset: 60,000 32x32 color images in 10 classes\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be in range [0, 1] for faster convergence\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# ---------------------- 2. Define the CNN Model ----------------------\n",
    "model = models.Sequential([\n",
    "    # First convolutional layer: 32 filters of size 3x3, ReLU activation\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "\n",
    "    # Max pooling: reduces spatial dimensions by 2\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional layer: 64 filters of size 3x3\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "\n",
    "    # Flatten: convert 2D feature maps into 1D feature vector\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Dense hidden layer with 64 units and ReLU activation\n",
    "    layers.Dense(64, activation='relu'),\n",
    "\n",
    "    # Output layer with 10 units (one for each CIFAR-10 class), softmax for multiclass classification\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# ---------------------- 3. Compile the Model ----------------------\n",
    "# Use Adam optimizer, sparse categorical crossentropy loss (for integer labels), and accuracy metric\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ---------------------- 4. Train the Model ----------------------\n",
    "# Train the model on training data for 5 epochs\n",
    "# Use 10% of the training data for validation\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5661b7",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe46fb",
   "metadata": {},
   "source": [
    "2. Image Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375346db",
   "metadata": {},
   "source": [
    "Image augmentation is a technique used in computer vision and deep learning to artificially expand the size and diversity of a training dataset by applying various transformations to the original images ‚Äî while preserving their labels.\n",
    "\n",
    "\n",
    "‚úÖ Why use image augmentation?\n",
    "\n",
    "To prevent overfitting by teaching the model to generalize better.\n",
    "\n",
    "To simulate real-world variations (e.g., rotations, lighting changes, flips).\n",
    "\n",
    "To improve robustness and accuracy on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38111a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ---------------------- 1. Create Image Data Generator ----------------------\n",
    "# This generator will augment image data by applying random transformations.\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,         # Randomly rotate images by up to 30 degrees\n",
    "    horizontal_flip=True,      # Randomly flip images horizontally (useful for symmetrical images)\n",
    "    zoom_range=0.2             # Randomly zoom in on images by up to 20%\n",
    ")\n",
    "\n",
    "# ---------------------- 2. Fit the Generator on Training Data ----------------------\n",
    "# Compute any required statistics for normalization or transformations (not mandatory here but often used).\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928d59d",
   "metadata": {},
   "source": [
    "3. Object Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1839d29",
   "metadata": {},
   "source": [
    "### üì¶ 3. **Object Detection** ‚Äî Explained Clearly\n",
    "\n",
    "**Object Detection** is a computer vision task that involves:\n",
    "1. **Detecting** the presence of objects in an image.\n",
    "2. **Classifying** each object (e.g., \"person\", \"car\", \"dog\").\n",
    "3. **Locating** each object using **bounding boxes**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå What makes Object Detection different?\n",
    "| Task              | Output                                       |\n",
    "|-------------------|----------------------------------------------|\n",
    "| Image Classification | \"Dog\" (whole image has a dog)               |\n",
    "| Object Detection     | \"Dog at (x1, y1, x2, y2)\", \"Cat at (...)\"  |\n",
    "| Semantic Segmentation| Per-pixel classification (no boxes)        |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Components:\n",
    "- **Bounding Box**: A rectangle that encloses the detected object.\n",
    "- **Class Label**: The type of object detected.\n",
    "- **Confidence Score**: How sure the model is about the detection.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Popular Object Detection Models:\n",
    "| Model     | Type         | Description                        |\n",
    "|-----------|--------------|------------------------------------|\n",
    "| YOLO      | One-stage     | Fast and accurate (You Only Look Once) |\n",
    "| SSD       | One-stage     | Real-time speed, good accuracy     |\n",
    "| Faster R-CNN | Two-stage | High accuracy, slower              |\n",
    "| DETR      | Transformer-based | Accurate and simple pipeline      |\n",
    "\n",
    "---\n",
    "\n",
    "### üñºÔ∏è Example Output:\n",
    "You give an image ‚Üí the model outputs:\n",
    "```json\n",
    "[\n",
    "  {\"label\": \"cat\", \"bbox\": [50, 30, 200, 180], \"confidence\": 0.92},\n",
    "  {\"label\": \"dog\", \"bbox\": [220, 80, 400, 300], \"confidence\": 0.88}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Use Cases:\n",
    "- Self-driving cars (detect pedestrians, vehicles)\n",
    "- Security surveillance\n",
    "- Healthcare (detect tumors in scans)\n",
    "- Retail (automated checkout)\n",
    "\n",
    "Would you like a simple code example using TensorFlow or YOLOv5 for object detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19757b7f",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c8799",
   "metadata": {},
   "source": [
    "4. Image Segmentation (U-Net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63336d",
   "metadata": {},
   "source": [
    "### üß† 4. **Image Segmentation (U-Net)** ‚Äî Explained Clearly\n",
    "\n",
    "**Image Segmentation** is a computer vision task where every pixel in an image is **classified into a category**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What It Does:\n",
    "- Unlike **object detection**, which draws bounding boxes...\n",
    "- **Image segmentation** gives **pixel-level precision** ‚Äî like coloring inside the lines!\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Types of Image Segmentation:\n",
    "| Type                     | Description                                   |\n",
    "|--------------------------|-----------------------------------------------|\n",
    "| **Semantic Segmentation** | Classifies pixels (e.g., car, road, sky)     |\n",
    "| **Instance Segmentation** | Distinguishes **instances** (e.g., 2 different dogs) |\n",
    "| **Panoptic Segmentation** | Combines both semantic + instance             |\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è U-Net Architecture (Popular for Segmentation)\n",
    "\n",
    "**U-Net** is a **Convolutional Neural Network (CNN)** designed for biomedical image segmentation. It's shaped like a \"U\" and has two main parts:\n",
    "\n",
    "#### üåÄ Encoder (Downsampling):\n",
    "- Extracts features using **Conv + MaxPool** layers\n",
    "- Like shrinking the image to understand **what** is in it\n",
    "\n",
    "#### üîÅ Bottleneck:\n",
    "- The compressed representation (deepest layer)\n",
    "\n",
    "#### üî∫ Decoder (Upsampling):\n",
    "- Uses **ConvTranspose** or upsampling to **rebuild** the image\n",
    "- Helps decide **where** the object is in the image\n",
    "- Uses **skip connections** from the encoder to improve precision\n",
    "\n",
    "```\n",
    "Input Image --> ‚Üì‚Üì‚Üì Encoder --> Bottleneck --> ‚Üë‚Üë‚Üë Decoder --> Segmented Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Output Example:\n",
    "Input: MRI scan  \n",
    "Output: Mask where the tumor area is white and the rest is black\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Use Cases:\n",
    "- **Medical imaging** (e.g., tumor segmentation)\n",
    "- **Satellite imagery** (e.g., segment roads, forests)\n",
    "- **Autonomous vehicles** (e.g., lane and pedestrian detection)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Outline (Keras U-Net)\n",
    "```python\n",
    "# Input: image (128x128x3)\n",
    "# Output: segmentation mask (128x128x1)\n",
    "\n",
    "inputs = tf.keras.Input((128, 128, 3))\n",
    "# Downsampling\n",
    "c1 = Conv2D(16, (3,3), activation='relu', padding='same')(inputs)\n",
    "p1 = MaxPooling2D((2,2))(c1)\n",
    "# Bottleneck\n",
    "b = Conv2D(64, (3,3), activation='relu', padding='same')(p1)\n",
    "# Upsampling\n",
    "u1 = UpSampling2D((2,2))(b)\n",
    "c2 = Conv2D(16, (3,3), activation='relu', padding='same')(u1)\n",
    "outputs = Conv2D(1, (1,1), activation='sigmoid')(c2)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "Would you like the **full U-Net implementation in TensorFlow/Keras** for a dataset like **Oxford Pets** or **lung segmentation**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def unet_model(input_shape):\n",
    "    # Input layer for RGB images of size 128x128\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Downsampling path (Encoder)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)  # Extract features\n",
    "    x = layers.MaxPooling2D()(x)  # Reduce spatial dimensions by 2\n",
    "\n",
    "    # Upsampling path (Decoder)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)  # Restore original size\n",
    "\n",
    "    # Output layer with 1 filter for binary segmentation, sigmoid for pixel-wise classification\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Instantiate the model with input shape (128, 128, 3)\n",
    "model = unet_model((128, 128, 3))\n",
    "\n",
    "# Compile the model with binary crossentropy loss for binary segmentation\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97294e",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ced9b4",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c2b07",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04ea54",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4592b7",
   "metadata": {},
   "source": [
    "1. Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c230e41",
   "metadata": {},
   "source": [
    "Option 1: TextVectorization Layer (for pipelines)\n",
    "\n",
    "The TextVectorization layer is used within TensorFlow models to preprocess text before feeding it into neural networks. It provides a convenient way to tokenize and vectorize text directly in the model's input pipeline.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Vocabulary Creation: It builds a vocabulary of all words or tokens from the text data.\n",
    "\n",
    "Text-to-Integer Conversion: It converts words into integer indices, allowing neural networks to process the text data.\n",
    "\n",
    "Handles Padding & Truncation: It can pad or truncate text to ensure that input sequences are of uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Sample input texts for text preprocessing\n",
    "texts = [\"TensorFlow is powerful\", \"NLP is cool\"]\n",
    "\n",
    "# Initialize the TextVectorization layer\n",
    "# - output_mode='int' means the output will be integers representing the index of words in the vocabulary\n",
    "# - max_tokens=100 limits the vocabulary size to the top 100 most frequent words in the dataset\n",
    "vectorizer = TextVectorization(output_mode='int', max_tokens=100)\n",
    "\n",
    "# Adapt the vectorizer on the input texts. This builds the vocabulary based on the texts.\n",
    "# The `adapt()` method updates the internal vocabulary of the TextVectorization layer based on the input texts.\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Applying the vectorizer to the input texts and printing the result\n",
    "# This will convert each word in the texts to its index in the vocabulary.\n",
    "print(vectorizer(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e0b5b",
   "metadata": {},
   "source": [
    "2. Embeddings\n",
    "\n",
    "Embeddings are a way to represent words, sentences, or other text-based data as dense, continuous vectors in a lower-dimensional space. Unlike traditional one-hot encoding, which results in sparse vectors, embeddings allow for a more compact and meaningful representation of words that captures semantic relationships between them.\n",
    "\n",
    "For example, words with similar meanings are often mapped to similar vectors in the embedding space (e.g., \"king\" and \"queen\" might be closer in vector space than \"king\" and \"apple\"). Embeddings are commonly used in natural language processing (NLP) tasks like sentiment analysis, machine translation, and text classification.\n",
    "\n",
    "Types of Embeddings:\n",
    "\n",
    "Word Embeddings: Represent individual words. Examples include Word2Vec, GloVe, FastText, etc.\n",
    "\n",
    "Sentence Embeddings: Represent entire sentences or phrases. Examples include BERT, GPT, or Universal Sentence Encoder.\n",
    "\n",
    "Pre-trained Embeddings: You can use embeddings that have been pre-trained on a large corpus of data and fine-tune them on your specific task.\n",
    "\n",
    "Trainable Embeddings: You can also train embeddings from scratch, especially for tasks where domain-specific embeddings are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer: Converts input integers into dense vectors\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=100),\n",
    "    \n",
    "    # Global Average Pooling: Averages the sequence of word vectors into a single vector\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    # Fully connected layer (Dense): Outputs a binary prediction (0 or 1)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8153bdf",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579617a",
   "metadata": {},
   "source": [
    "3. LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM is a type of Recurrent Neural Network (RNN) designed to capture long-term dependencies in sequential data. Unlike traditional RNNs, LSTMs are specifically designed to address the vanishing gradient problem, which allows them to learn from long sequences of data more effectively.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "Memory Cells: LSTM units have memory cells that allow them to remember information for long periods.\n",
    "\n",
    "Gates: LSTMs use three gates (input, output, and forget) to regulate the flow of information and learn which data to keep and which to discard.\n",
    "\n",
    "Structure of LSTM:\n",
    "\n",
    "Forget Gate: Decides what information from the cell state should be discarded.\n",
    "\n",
    "Input Gate: Determines what new information should be added to the cell state.\n",
    "\n",
    "Output Gate: Decides what part of the cell state should be outputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78304eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    # Embedding layer: converts integer sequences into dense vector representations (word embeddings)\n",
    "    tf.keras.layers.Embedding(10000, 64),  # 10000 is the size of the vocabulary, 64 is the dimension of the embedding vectors\n",
    "\n",
    "    # LSTM (Long Short-Term Memory) layer: processes the sequential data, capturing long-term dependencies in the input sequence\n",
    "    tf.keras.layers.LSTM(64),  # 64 units in the LSTM layer, outputting a 64-dimensional vector\n",
    "\n",
    "    # Dense layer: final output layer for binary classification\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # 1 unit (output), sigmoid activation for binary classification (output between 0 and 1)\n",
    "])\n",
    "\n",
    "# Compile the model with the Adam optimizer and binary crossentropy loss function for binary classification\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Example data for training (replace with actual data)\n",
    "# x_train = [...]  # Input text data (integer-encoded sequences)\n",
    "# y_train = [...]  # Labels (0 or 1 for binary classification)\n",
    "\n",
    "# Train the model with the training data (adjust epochs and batch_size as needed)\n",
    "# model.fit(x_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5614d46",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efda23",
   "metadata": {},
   "source": [
    "4. Using BERT via TensorFlow Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ada328",
   "metadata": {},
   "source": [
    "To use BERT (Bidirectional Encoder Representations from Transformers) for Natural Language Processing tasks via TensorFlow Hub, we can load a pre-trained BERT model and fine-tune it for a downstream task like text classification.\n",
    "\n",
    "Here's an example of how you can use BERT with TensorFlow Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Load BERT Preprocessing and Encoder Models from TensorFlow Hub\n",
    "# The preprocessing model will handle the tokenization and conversion of input text into BERT-compatible format.\n",
    "bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "\n",
    "# The encoder model is the actual BERT model that generates contextual embeddings from the preprocessed input.\n",
    "bert_encoder = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# 2. Define Input Layer for the Model\n",
    "# The model expects input as a string (text). Here, we define the input layer for the model.\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "# 3. Preprocess the Input Text\n",
    "# The BERT preprocessing model converts the input text into the appropriate tokenized format \n",
    "# that BERT expects (i.e., token ids, segment ids, etc.).\n",
    "preprocessed = bert_preprocess(text_input)\n",
    "\n",
    "# 4. Pass the Preprocessed Text through the BERT Encoder\n",
    "# The BERT encoder generates contextualized embeddings for the input text.\n",
    "# The 'pooled_output' is typically used for classification tasks, representing a fixed-length vector \n",
    "# derived from the entire sequence of tokens.\n",
    "outputs = bert_encoder(preprocessed)['pooled_output']\n",
    "\n",
    "# 5. Build the Model\n",
    "# Create the final model by specifying the input and output layers. The output of the model \n",
    "# will be the BERT 'pooled_output', which can be used for downstream tasks (e.g., classification).\n",
    "model = tf.keras.Model(text_input, outputs)\n",
    "\n",
    "# Model summary (optional) to check the structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e9488",
   "metadata": {},
   "source": [
    "| Task | Key APIs | Use Case |\n",
    "|------|----------|----------|\n",
    "| Image Classification | `Conv2D`, `MaxPooling2D` | Object recognition |\n",
    "| Image Augmentation | `ImageDataGenerator`, `tf.image` | Overfitting prevention |\n",
    "| Segmentation | Custom U-Net | Medical, satellite |\n",
    "| Tokenization | `TextVectorization`, `Tokenizer` | NLP preprocessing |\n",
    "| Embedding | `Embedding` layer, TF Hub | Word representation |\n",
    "| Sequence Models | `LSTM`, `TransformerBlock` | Language modeling |\n",
    "| Pretrained NLP | `TF Hub BERT` | QA, classification |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
